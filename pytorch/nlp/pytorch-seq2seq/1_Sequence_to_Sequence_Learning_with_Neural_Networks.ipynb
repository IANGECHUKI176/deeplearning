{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IANGECHUKI176/deeplearning/blob/main/pytorch/nlp/pytorch-seq2seq/1_Sequence_to_Sequence_Learning_with_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqDlX91vokGT"
      },
      "source": [
        "Reference: [Bentrevett Github](https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb)\n",
        "\n",
        "paper : [Sequence to Sequence Learning\n",
        "with Neural Networks](https://arxiv.org/abs/1409.3215)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o2h3DFRs9Gz"
      },
      "source": [
        "Preparing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpQoxFGNi257"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import math\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import vocab\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "import io"
      ],
      "metadata": {
        "id": "ZDG1Sdc3Vog8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !python -m spacy download en_core_web_sm\n",
        "# !python -m spacy download de_core_news_sm"
      ],
      "metadata": {
        "id": "hxsLMfJgVzKN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
        "train_urls = ('train.de.gz', 'train.en.gz')\n",
        "val_urls = ('val.de.gz', 'val.en.gz')\n",
        "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
        "\n",
        "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
        "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
        "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n",
        "\n",
        "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "def build_vocab(filepath, tokenizer):\n",
        "    counter = Counter()\n",
        "    with io.open(filepath, encoding=\"utf8\") as f:\n",
        "        for string_ in f:\n",
        "            counter.update(tokenizer(string_))\n",
        "    v2 = vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "    v2.set_default_index(0)\n",
        "    return v2\n",
        "\n",
        "de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
        "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)\n",
        "\n",
        "def data_process(filepaths):\n",
        "    raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "    raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "    data = []\n",
        "    for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
        "        de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)],\n",
        "                                dtype=torch.long)\n",
        "        en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)],\n",
        "                                dtype=torch.long)\n",
        "        data.append((de_tensor_, en_tensor_))\n",
        "    return data\n",
        "\n",
        "train_data = data_process(train_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "test_data = data_process(test_filepaths)"
      ],
      "metadata": {
        "id": "uNxzWZmNWFqf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a156ac13-197a-4b13-e247-cb7431c0a89c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 637k/637k [00:00<00:00, 10.0MB/s]\n",
            "100%|██████████| 569k/569k [00:00<00:00, 7.06MB/s]\n",
            "100%|██████████| 24.7k/24.7k [00:00<00:00, 2.06MB/s]\n",
            "100%|██████████| 21.6k/21.6k [00:00<00:00, 3.19MB/s]\n",
            "100%|██████████| 22.9k/22.9k [00:00<00:00, 2.35MB/s]\n",
            "100%|██████████| 21.1k/21.1k [00:00<00:00, 3.34MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "PAD_IDX = de_vocab['<pad>']\n",
        "BOS_IDX = de_vocab['<bos>']\n",
        "EOS_IDX = de_vocab['<eos>']\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "def generate_batch(data_batch):\n",
        "  de_batch, en_batch = [], []\n",
        "  for (de_item, en_item) in data_batch:\n",
        "    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
        "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
        "  return de_batch, en_batch\n",
        "\n",
        "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "                       shuffle=True, collate_fn=generate_batch)\n"
      ],
      "metadata": {
        "id": "Zf4AffLZWGpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YM0aR7nMYaYL"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,input_dim,embed_dim,hidden_dim,n_layers,dropout):\n",
        "        super(Encoder,self).__init__()\n",
        "        self.hidden_dim  = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = nn.Embedding(input_dim,embed_dim)\n",
        "        self.rnn = nn.LSTM(embed_dim,hidden_dim,n_layers,dropout = dropout)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,src):\n",
        "        #src = [src len, batch size]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "\n",
        "        out,(hidden,cell) = self.rnn(embedded)\n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        #outputs are always from the top hidden layer\n",
        "        return hidden,cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dPFPAvXxWC2G"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,output_dim,embed_dim,hidden_dim,n_layers,dropout):\n",
        "        super(Decoder,self).__init__()\n",
        "        self.hidden_dim  = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim,embed_dim)\n",
        "        self.rnn = nn.LSTM(embed_dim,hidden_dim,n_layers,dropout = dropout)\n",
        "\n",
        "        self.fc_out = nn.Linear(hidden_dim,output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,input,hidden,cell):\n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        #n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "        input = input.unsqueeze(0)\n",
        "        #input = [1,batch size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        #embedded = [1,batch size,embed_dim]\n",
        "        output , (hidden,cell) = self.rnn(embedded,(hidden,cell))\n",
        "\n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #cell = [n layers, batch size, hid dim]\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        #prediction = [batch size, output dim]\n",
        "        return prediction,hidden,cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OOsFWkYhUXX"
      },
      "source": [
        "Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4Qi1fEvhPHo"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,encoder,decoder,device):\n",
        "        super(Seq2Seq,self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "\n",
        "        assert encoder.hidden_dim == decoder.hidden_dim ,\"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers ,'No of layer in encoder and decoder must be equal'\n",
        "    def forward(self,src,trg,teacher_forcing_ratio = 0.5):\n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = decoder.output_dim\n",
        "\n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len,batch_size,trg_vocab_size).to(self.device)\n",
        "\n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden,cell = self.encoder(src)\n",
        "\n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "\n",
        "        for t in range(1,trg_len):\n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            prediction,hidden,cell = self.decoder(input,hidden,cell)\n",
        "            outputs[t] = prediction\n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = prediction.argmax(1)\n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfXXMyJGk-B7"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(de_vocab)\n",
        "OUTPUT_DIM = len(en_vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "ENC_HID_DIM = 512\n",
        "DEC_HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "N_LAYERS = 2\n",
        "\n",
        "\n",
        "\n",
        "encoder = Encoder(INPUT_DIM,ENC_EMB_DIM,ENC_HID_DIM,N_LAYERS,ENC_DROPOUT)\n",
        "decoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(encoder,decoder,device).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPbZ6WL60Ogj",
        "outputId": "ac8e9d7c-f2b3-4735-b7c5-8d83c0392b96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(19215, 128)\n",
              "    (rnn): LSTM(128, 256, num_layers=2, dropout=0.6)\n",
              "    (dropout): Dropout(p=0.6, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(10838, 128)\n",
              "    (rnn): LSTM(128, 256, num_layers=2, dropout=0.6)\n",
              "    (fc_out): Linear(in_features=256, out_features=10838, bias=True)\n",
              "    (dropout): Dropout(p=0.6, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "def init_weights(m: nn.Module):\n",
        "    for name, param in m.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "        else:\n",
        "            nn.init.constant_(param.data, 0)\n",
        "\n",
        "model.apply(init_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHjmO76r1AVc",
        "outputId": "ae0c6bbd-c5dd-454e-e79b-3425c2717ea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 8,475,350 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model:nn.Module):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ncsHB2GB2PHh"
      },
      "outputs": [],
      "source": [
        "TRG_PAD_IDX = en_vocab.get_stoi()['<pad>']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DdpQarl3pbg"
      },
      "outputs": [],
      "source": [
        "def train(model: nn.Module,\n",
        "          iterator: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          criterion: nn.Module,\n",
        "          clip: float):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for _, (src, trg) in enumerate(tqdm(iterator)):\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src, trg)\n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "        output = output[1:].view(-1, output.shape[-1])\n",
        "        trg = trg[1:].view(-1)\n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZN7ToMYAeW0"
      },
      "outputs": [],
      "source": [
        "def evaluate(model: nn.Module,\n",
        "             iterator: torch.utils.data.DataLoader,\n",
        "             criterion: nn.Module):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for _, (src, trg) in enumerate(tqdm(iterator)):\n",
        "            src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output = output[1:].view(-1, output.shape[-1])\n",
        "            trg = trg[1:].view(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mey8ocVa6un4"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time: int,\n",
        "               end_time: int):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEBKzdbGBPos",
        "outputId": "6bf03225-cbf0-4599-c43a-8421f2522e1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 227/227 [00:56<00:00,  4.03it/s]\n",
            "100%|██████████| 8/8 [00:01<00:00,  6.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 0m 57s\n",
            "\tTrain Loss: 5.391 | Train PPL: 219.363\n",
            "\t Val. Loss: 5.019 |  Val. PPL: 151.289\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 227/227 [00:56<00:00,  4.03it/s]\n",
            "100%|██████████| 8/8 [00:01<00:00,  7.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 02 | Time: 0m 57s\n",
            "\tTrain Loss: 4.861 | Train PPL: 129.181\n",
            "\t Val. Loss: 4.983 |  Val. PPL: 145.951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 227/227 [00:55<00:00,  4.09it/s]\n",
            "100%|██████████| 8/8 [00:01<00:00,  6.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 03 | Time: 0m 56s\n",
            "\tTrain Loss: 4.583 | Train PPL:  97.826\n",
            "\t Val. Loss: 4.910 |  Val. PPL: 135.703\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 227/227 [00:56<00:00,  4.04it/s]\n",
            "100%|██████████| 8/8 [00:01<00:00,  6.82it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 04 | Time: 0m 57s\n",
            "\tTrain Loss: 4.389 | Train PPL:  80.567\n",
            "\t Val. Loss: 4.843 |  Val. PPL: 126.859\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 227/227 [00:56<00:00,  4.04it/s]\n",
            "100%|██████████| 8/8 [00:01<00:00,  6.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 05 | Time: 0m 57s\n",
            "\tTrain Loss: 4.276 | Train PPL:  71.963\n",
            "\t Val. Loss: 4.765 |  Val. PPL: 117.386\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 227/227 [00:56<00:00,  4.05it/s]\n",
            "100%|██████████| 8/8 [00:01<00:00,  6.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 06 | Time: 0m 57s\n",
            "\tTrain Loss: 4.156 | Train PPL:  63.814\n",
            "\t Val. Loss: 4.814 |  Val. PPL: 123.167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 227/227 [00:56<00:00,  4.04it/s]\n",
            "100%|██████████| 8/8 [00:01<00:00,  6.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 07 | Time: 0m 57s\n",
            "\tTrain Loss: 4.057 | Train PPL:  57.820\n",
            "\t Val. Loss: 4.664 |  Val. PPL: 106.033\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 227/227 [00:56<00:00,  4.05it/s]\n",
            "100%|██████████| 8/8 [00:01<00:00,  6.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 08 | Time: 0m 57s\n",
            "\tTrain Loss: 3.968 | Train PPL:  52.902\n",
            "\t Val. Loss: 4.635 |  Val. PPL: 103.035\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 227/227 [00:56<00:00,  4.01it/s]\n",
            "100%|██████████| 8/8 [00:01<00:00,  6.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 09 | Time: 0m 57s\n",
            "\tTrain Loss: 3.916 | Train PPL:  50.191\n",
            "\t Val. Loss: 4.632 |  Val. PPL: 102.716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 227/227 [00:55<00:00,  4.08it/s]\n",
            "100%|██████████| 8/8 [00:01<00:00,  6.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10 | Time: 0m 57s\n",
            "\tTrain Loss: 3.842 | Train PPL:  46.617\n",
            "\t Val. Loss: 4.529 |  Val. PPL:  92.680\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [00:01<00:00,  6.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Test Loss: 4.518 | Test PPL:  91.616 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iter, criterion)\n",
        "    end_time = time.time()\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(valid_loss)\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "    torch.save(model.state_dict(), 'tut1-model.pt')\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "test_loss = evaluate(model, test_iter, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('tut1-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iter, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YY0U0Nq7g1z6",
        "outputId": "9ac0615a-00e4-40e6-9193-caf4ea496201"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8/8 [00:01<00:00,  6.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Test Loss: 4.520 | Test PPL:  91.795 |\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the translation function\n",
        "def translate_sentence(model, src_sentence,device, max_len=50):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        if isinstance(src_sentence, str):\n",
        "            tokens = de_tokenizer(src_sentence)\n",
        "        else:\n",
        "            tokens = src_sentence\n",
        "\n",
        "\n",
        "        tokens = ['<bos>'] + tokens + ['<eos>']\n",
        "        src_indexes = de_vocab.lookup_indices(tokens)\n",
        "\n",
        "        src_tensor = torch.tensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            hidden,cell = model.encoder(src_tensor)\n",
        "\n",
        "        BOS_INDEX = en_vocab.get_stoi()[\"<bos>\"]\n",
        "\n",
        "        trg_indexes = [BOS_INDEX]\n",
        "\n",
        "        for i in range(max_len):\n",
        "\n",
        "            trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "            with torch.no_grad():\n",
        "                prediction,hidden,cell = model.decoder(trg_tensor,hidden,cell)\n",
        "\n",
        "\n",
        "            pred_token = prediction.argmax(1).item()\n",
        "\n",
        "            trg_indexes.append(pred_token)\n",
        "\n",
        "            if pred_token == en_vocab.get_stoi()[\"<eos>\"]:\n",
        "                break\n",
        "\n",
        "    trg_tokens = en_vocab.lookup_tokens(trg_indexes)\n",
        "\n",
        "    return trg_tokens[1:]"
      ],
      "metadata": {
        "id": "c8-JxCWdal9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_data(filepaths): #test_filepaths\n",
        "    raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "    raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "    return raw_de_iter,raw_en_iter"
      ],
      "metadata": {
        "id": "oGceZBRtam3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_x,test_y = get_test_data(test_filepaths)\n",
        "src_arr = []\n",
        "trg_arr = []\n",
        "for x,y in zip(test_x,test_y):\n",
        "    src_arr.append(x)\n",
        "    trg_arr.append(y)"
      ],
      "metadata": {
        "id": "SnJl9CXUauRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_idx = 10\n",
        "src = src_arr[example_idx]\n",
        "trg = trg_arr[example_idx]\n",
        "\n",
        "translation = translate_sentence(model,src, device)\n",
        "print('src:',src)\n",
        "print('trg:',trg)\n",
        "print(f'predicted trg = {translation}')"
      ],
      "metadata": {
        "id": "VkoUsbvRaw0v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7729b243-d516-4c71-c272-946278b919b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src: Eine Mutter und ihr kleiner Sohn genießen einen schönen Tag im Freien.\n",
            "\n",
            "trg: A mother and her young song enjoying a beautiful day outside.\n",
            "\n",
            "predicted trg = ['A', 'person', 'in', 'a', 'a', 'a', 'a', '.', '\\n', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_idx = 12\n",
        "src = src_arr[example_idx]\n",
        "trg = trg_arr[example_idx]\n",
        "\n",
        "translation = translate_sentence(model,src, device)\n",
        "print('src:',src)\n",
        "print('trg:',trg)\n",
        "print(f'predicted trg = {translation}')"
      ],
      "metadata": {
        "id": "opUhyFf4a10w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2b467d2-3f8a-4b07-fe76-692bb0cab440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src: Eine Frau, die in einer Küche eine Schale mit Essen hält.\n",
            "\n",
            "trg: A woman holding a bowl of food in a kitchen.\n",
            "\n",
            "predicted trg = ['A', 'woman', 'is', 'a', 'a', 'a', 'a', 'a', '.', '\\n', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_idx = 14\n",
        "src = src_arr[example_idx]\n",
        "trg = trg_arr[example_idx]\n",
        "\n",
        "translation = translate_sentence(model,src, device)\n",
        "print('src:',src)\n",
        "print('trg:',trg)\n",
        "print(f'predicted trg = {translation}')"
      ],
      "metadata": {
        "id": "_PCQ725ha3e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09035d29-446c-429a-d5db-433ced33f93f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src: Drei Leute sitzen in einer Höhle.\n",
            "\n",
            "trg: Three people sit in a cave.\n",
            "\n",
            "predicted trg = ['Two', 'people', 'are', 'a', '.', '\\n', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_idx = 16\n",
        "src = src_arr[example_idx]\n",
        "trg = trg_arr[example_idx]\n",
        "\n",
        "translation = translate_sentence(model,src, device)\n",
        "print('src:',src)\n",
        "print('trg:',trg)\n",
        "print(f'predicted trg = {translation}')"
      ],
      "metadata": {
        "id": "P6c-Z4tDa4GH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b3b249f-d17e-40ba-c025-cf7e7b4d544c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src: Eine Blondine hält mit einem Mann im Sand Händchen.\n",
            "\n",
            "trg: A blond holding hands with a guy in the sand.\n",
            "\n",
            "predicted trg = ['A', 'woman', 'in', 'a', 'blue', 'shirt', 'is', 'a', 'a', '.', '.', '\\n', '<eos>']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}