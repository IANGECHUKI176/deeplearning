{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IANGECHUKI176/deeplearning/blob/main/pytorch/nlp/pytorch-seq2seq/2_Learning_Phrase_Representations_using_RNN_Encoder_Decoder_for_Statistical_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference [Bentrevett Github](https://github.com/bentrevett/pytorch-seq2seq/blob/master/2%20-%20Learning%20Phrase%20Representations%20using%20RNN%20Encoder-Decoder%20for%20Statistical%20Machine%20Translation.ipynb)\n",
        "\n",
        "Paper: [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/pdf/1406.1078v3.pdf)"
      ],
      "metadata": {
        "id": "kWoY6eSMBZY4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmVo6ib8jiJj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "#pytorch/nlp/pytorch-seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "import torch\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import vocab\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "import io"
      ],
      "metadata": {
        "id": "vDTjcUt9LL2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download de_core_news_sm"
      ],
      "metadata": {
        "id": "XExp3mxYLcla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
        "train_urls = ('train.de.gz', 'train.en.gz')\n",
        "val_urls = ('val.de.gz', 'val.en.gz')\n",
        "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
        "\n",
        "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
        "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
        "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]\n",
        "\n",
        "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
        "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')\n",
        "\n",
        "def build_vocab(filepath, tokenizer):\n",
        "    counter = Counter()\n",
        "    with io.open(filepath, encoding=\"utf8\") as f:\n",
        "        for string_ in f:\n",
        "            counter.update(tokenizer(string_))\n",
        "    v2 = vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
        "    v2.set_default_index(0)\n",
        "    return v2\n",
        "\n",
        "de_vocab = build_vocab(train_filepaths[0], de_tokenizer)\n",
        "en_vocab = build_vocab(train_filepaths[1], en_tokenizer)\n",
        "\n",
        "def data_process(filepaths):\n",
        "    raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "    raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "    data = []\n",
        "    for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
        "        de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)],\n",
        "                                dtype=torch.long)\n",
        "        en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)],\n",
        "                                dtype=torch.long)\n",
        "        data.append((de_tensor_, en_tensor_))\n",
        "    return data\n",
        "\n",
        "train_data = data_process(train_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "test_data = data_process(test_filepaths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_doopHSwC0Dr",
        "outputId": "2e020147-40f5-407d-badc-1d75211a1e27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 637k/637k [00:00<00:00, 11.2MB/s]\n",
            "100%|██████████| 569k/569k [00:00<00:00, 11.5MB/s]\n",
            "100%|██████████| 24.7k/24.7k [00:00<00:00, 7.05MB/s]\n",
            "100%|██████████| 21.6k/21.6k [00:00<00:00, 2.39MB/s]\n",
            "100%|██████████| 22.9k/22.9k [00:00<00:00, 7.89MB/s]\n",
            "100%|██████████| 21.1k/21.1k [00:00<00:00, 36.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 128\n",
        "PAD_IDX = de_vocab['<pad>']\n",
        "BOS_IDX = de_vocab['<bos>']\n",
        "EOS_IDX = de_vocab['<eos>']\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "def generate_batch(data_batch):\n",
        "  de_batch, en_batch = [], []\n",
        "  for (de_item, en_item) in data_batch:\n",
        "    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
        "  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
        "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
        "  return de_batch, en_batch\n",
        "\n",
        "train_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "valid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n",
        "                        shuffle=True, collate_fn=generate_batch)\n",
        "test_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n",
        "                       shuffle=True, collate_fn=generate_batch)\n"
      ],
      "metadata": {
        "id": "nzMbhKb2LlM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self,input_dim,embed_dim,hidden_dim,dropout):\n",
        "        super(Encoder,self).__init__()\n",
        "\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding = nn.Embedding(input_dim,embed_dim)\n",
        "        self.rnn = nn.GRU(embed_dim,hidden_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,src):\n",
        "        #src = [seq len,batch_size]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "\n",
        "        #embedded = [src len,batch size , embed dim]\n",
        "        outputs,hidden = self.rnn(embedded)\n",
        "        #outputs =[src len,batch size,hidden dim * n directions]\n",
        "        #hidden = [n layers * n directions,batch size,hidden dim]\n",
        "\n",
        "        #outputs are always from the top hidden layer\n",
        "        return hidden\n"
      ],
      "metadata": {
        "id": "bTpHAaULHUEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self,output_dim,emb_dim,hid_dim,dropout):\n",
        "        super(Decoder,self).__init__()\n",
        "\n",
        "        self.hid_dim = hid_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.embedding = nn.Embedding(output_dim,emb_dim)\n",
        "        self.rnn = nn.GRU(emb_dim + hid_dim ,hid_dim)\n",
        "        self.fc_out = nn.Linear(emb_dim + hid_dim * 2,output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self,input,hidden,context):\n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions ,batch size , hidden dim]\n",
        "        #context = [n layers * n directions ,batch size , hidden dim]\n",
        "\n",
        "        #n layers and n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden  = [1,batch size , hidden dim]\n",
        "        #context = [1,batch size , hidden dim]\n",
        "\n",
        "        input = input.unsqueeze(0)\n",
        "\n",
        "        #input = [1,batch size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        #[1,batch size,emb dim]\n",
        "\n",
        "        emb_con = torch.cat([embedded,context],dim = 2)\n",
        "        # #emb_con = [1,batch_size,emb_dim + hid dim]\n",
        "\n",
        "        outputs,hidden = self.rnn(emb_con,hidden)\n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "\n",
        "        #seq len, n layers and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [1, batch size, hid dim]\n",
        "\n",
        "        output = torch.cat([embedded.squeeze(0),hidden.squeeze(0),context.squeeze(0)],dim = 1)\n",
        "        #output = [batch size, emb dim + hid dim * 2]\n",
        "        prediction = self.fc_out(output)\n",
        "        #prediction = [batch size, output dim]\n",
        "        return prediction,hidden"
      ],
      "metadata": {
        "id": "-sAnz9I9Pjbt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self,encoder,decoder):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self,src,trg,teacher_forcing_ratio = 0.5):\n",
        "        #src = [seq len ,batch size]\n",
        "        #trg = [seq len ,batch size]\n",
        "\n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        #tensor to store decoded outputs\n",
        "        outputs = torch.zeros(trg_len,batch_size,trg_vocab_size) .to(device)\n",
        "\n",
        "        #last hidden state of the encoder is the context\n",
        "        context = self.encoder(src)\n",
        "\n",
        "        #context is also used as the initial hidden state of the decoder\n",
        "        hidden = context\n",
        "\n",
        "        input = trg[0,:]\n",
        "\n",
        "        for t in range(1,trg_len):\n",
        "            #insert input token embedding, previous hidden state and the context state\n",
        "            #receive output tensor (predictions) and new hidden state\n",
        "            output,hidden = self.decoder(input,hidden,context)\n",
        "\n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t]  = output\n",
        "\n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        return outputs"
      ],
      "metadata": {
        "id": "uxSZ2L9gpDTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM =  len(de_vocab)\n",
        "OUTPUT_DIM = len(en_vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, ENC_DROPOUT)\n",
        "\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, DEC_DROPOUT)\n",
        "model = Seq2Seq(enc,dec).to(device)"
      ],
      "metadata": {
        "id": "KkTHAYrziklz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m:nn.Module):\n",
        "    for name,param in m.named_parameters():\n",
        "        nn.init.normal_(param.data,mean = 0,std = 0.01)\n",
        "\n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q60IDU4ztvr3",
        "outputId": "efdb97b6-2ced-4429-f7d3-240b017bba04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(19215, 256)\n",
              "    (rnn): GRU(256, 512)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(10838, 256)\n",
              "    (rnn): GRU(768, 512)\n",
              "    (fc_out): Linear(in_features=1280, out_features=10838, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model:nn.Module):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_cCHvQjueow",
        "outputId": "d580c38e-22ec-4b4d-ac00-7f510585d775"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 24,728,918 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "BS2J9vY2vEyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRG_PAD_IDX = en_vocab.get_stoi()['<pad>']\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "id": "sq35XJ4GvTlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "YFNDiu4_EUKQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model:nn.Module,\n",
        "          iterator : torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          criterion: nn.Module,\n",
        "          clip: float\n",
        "          ):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for _ ,(src,trg) in enumerate(tqdm(iterator)):\n",
        "        src,trg = src.to(device),trg.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(src,trg)\n",
        "        #trg = [trg len, batch size]\n",
        "        #output = [trg len, batch size, output dim]\n",
        "\n",
        "        output = output[1:].view(-1,output.shape[-1])\n",
        "        trg = trg[1:].view(-1)\n",
        "\n",
        "        #trg = [(trg len - 1) * batch size]\n",
        "        #output = [(trg len - 1) * batch size, output dim]\n",
        "        loss = criterion(output,trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "9Dg8w7ZIv0tQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model : nn.Module,\n",
        "             iterator:torch.utils.data.DataLoader,\n",
        "             criterion : nn.Module\n",
        "             ):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _,(src,trg) in enumerate(tqdm(iterator)):\n",
        "            src,trg = src.to(device),trg.to(device)\n",
        "\n",
        "            output = model(src, trg, 0) #turn off teacher forcing\n",
        "            #trg = [trg len, batch size]\n",
        "            #output = [trg len, batch size, output dim]\n",
        "\n",
        "            output = output[1:].view(-1,output.shape[-1])\n",
        "            target = trg[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output,target)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)"
      ],
      "metadata": {
        "id": "Sx0uMESoybWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def epoch_time(start_time: int,\n",
        "               end_time: int):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "9zUsiQTqzjYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N_EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iter, criterion)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(valid_loss)\n",
        "\n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut2-model.pt')\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "test_loss = evaluate(model, test_iter, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "metadata": {
        "id": "w_dRw5y2zkG3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71c1ea62-3e7e-4b9a-f273-f49de9c5929a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 227/227 [01:17<00:00,  2.95it/s]\n",
            "100%|██████████| 8/8 [00:01<00:00,  5.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 1m 18s\n",
            "\tTrain Loss: 4.180 | Train PPL:  65.335\n",
            "\t Val. Loss: 4.820 |  Val. PPL: 124.009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 227/227 [01:18<00:00,  2.89it/s]\n",
            "100%|██████████| 8/8 [00:01<00:00,  5.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 02 | Time: 1m 19s\n",
            "\tTrain Loss: 3.857 | Train PPL:  47.311\n",
            "\t Val. Loss: 4.518 |  Val. PPL:  91.696\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 227/227 [01:16<00:00,  2.97it/s]\n",
            "100%|██████████| 8/8 [00:01<00:00,  4.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 03 | Time: 1m 18s\n",
            "\tTrain Loss: 3.500 | Train PPL:  33.108\n",
            "\t Val. Loss: 4.258 |  Val. PPL:  70.701\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 227/227 [01:18<00:00,  2.88it/s]\n",
            "100%|██████████| 8/8 [00:01<00:00,  5.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 04 | Time: 1m 20s\n",
            "\tTrain Loss: 3.127 | Train PPL:  22.810\n",
            "\t Val. Loss: 4.085 |  Val. PPL:  59.425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 227/227 [01:17<00:00,  2.95it/s]\n",
            "100%|██████████| 8/8 [00:01<00:00,  5.90it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 05 | Time: 1m 18s\n",
            "\tTrain Loss: 2.829 | Train PPL:  16.929\n",
            "\t Val. Loss: 4.056 |  Val. PPL:  57.744\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 227/227 [01:17<00:00,  2.94it/s]\n",
            "100%|██████████| 8/8 [00:01<00:00,  5.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 06 | Time: 1m 18s\n",
            "\tTrain Loss: 2.535 | Train PPL:  12.612\n",
            "\t Val. Loss: 3.896 |  Val. PPL:  49.229\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 83%|████████▎ | 189/227 [01:03<00:14,  2.60it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_losses)\n",
        "plt.plot(val_losses)"
      ],
      "metadata": {
        "id": "pX6FB3B5GIK4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the translation function\n",
        "def translate_sentence(model, src_sentence,device, max_len=50):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        if isinstance(src_sentence, str):\n",
        "            tokens = de_tokenizer(src_sentence)\n",
        "        else:\n",
        "            tokens = src_sentence\n",
        "\n",
        "\n",
        "        tokens = ['<bos>'] + tokens + ['<eos>']\n",
        "        src_indexes = de_vocab.lookup_indices(tokens)\n",
        "\n",
        "        src_tensor = torch.tensor(src_indexes).unsqueeze(1).to(device)\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            context = model.encoder(src_tensor)\n",
        "\n",
        "\n",
        "        BOS_INDEX = en_vocab.get_stoi()[\"<bos>\"]\n",
        "\n",
        "        trg_indexes = [BOS_INDEX]\n",
        "\n",
        "        hidden = context\n",
        "        for i in range(max_len):\n",
        "\n",
        "            trg_tensor = torch.LongTensor([trg_indexes[-1]]).to(device)\n",
        "            with torch.no_grad():\n",
        "                output, hidden = model.decoder(trg_tensor, hidden, context)\n",
        "\n",
        "\n",
        "            pred_token = output.argmax(1).item()\n",
        "\n",
        "            trg_indexes.append(pred_token)\n",
        "\n",
        "            if pred_token == en_vocab.get_stoi()[\"<eos>\"]:\n",
        "                break\n",
        "\n",
        "    trg_tokens = en_vocab.lookup_tokens(trg_indexes)\n",
        "\n",
        "    return trg_tokens[1:]"
      ],
      "metadata": {
        "id": "NPCdXFH7Nomb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_data(filepaths): #test_filepaths\n",
        "    raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "    raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "    return raw_de_iter,raw_en_iter"
      ],
      "metadata": {
        "id": "2AlUS6HmNp0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_x,test_y = get_test_data(test_filepaths)\n",
        "src_arr = []\n",
        "trg_arr = []\n",
        "for x,y in zip(test_x,test_y):\n",
        "    src_arr.append(x)\n",
        "    trg_arr.append(y)"
      ],
      "metadata": {
        "id": "BiydpuEzNvdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_idx = 10\n",
        "src = src_arr[example_idx]\n",
        "trg = trg_arr[example_idx]\n",
        "\n",
        "translation = translate_sentence(model,src, device)\n",
        "print('src:',src)\n",
        "print('trg:',trg)\n",
        "print(f'predicted trg = {translation}')"
      ],
      "metadata": {
        "id": "tVtvrJiSNx31",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fa81d86-aeb4-48e8-c7d9-df01d466599d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src: Eine Mutter und ihr kleiner Sohn genießen einen schönen Tag im Freien.\n",
            "\n",
            "trg: A mother and her young song enjoying a beautiful day outside.\n",
            "\n",
            "predicted trg = ['A', 'mother', 'and', 'her', 'daughter', 'enjoy', 'a', 'nice', 'day', 'day', '.', '\\n', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_idx = 12\n",
        "src = src_arr[example_idx]\n",
        "trg = trg_arr[example_idx]\n",
        "\n",
        "translation = translate_sentence(model,src, device)\n",
        "print('src:',src)\n",
        "print('trg:',trg)\n",
        "print(f'predicted trg = {translation}')"
      ],
      "metadata": {
        "id": "a6XKg1cgN1li",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87c12c83-8c96-47aa-a4e2-e9882f9c404d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src: Eine Frau, die in einer Küche eine Schale mit Essen hält.\n",
            "\n",
            "trg: A woman holding a bowl of food in a kitchen.\n",
            "\n",
            "predicted trg = ['A', 'woman', 'is', 'in', 'a', 'kitchen', 'kitchen', 'holding', 'a', 'dish', 'of', 'food', '.', '\\n', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_idx = 14\n",
        "src = src_arr[example_idx]\n",
        "trg = trg_arr[example_idx]\n",
        "\n",
        "translation = translate_sentence(model,src, device)\n",
        "print('src:',src)\n",
        "print('trg:',trg)\n",
        "print(f'predicted trg = {translation}')"
      ],
      "metadata": {
        "id": "gP1hr_auN50U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b55c81a4-1d28-41a5-85f3-32c38fb494c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src: Drei Leute sitzen in einer Höhle.\n",
            "\n",
            "trg: Three people sit in a cave.\n",
            "\n",
            "predicted trg = ['Three', 'people', 'sit', 'in', 'a', 'wooded', 'area', '.', '\\n', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_idx = 15\n",
        "src = src_arr[example_idx]\n",
        "trg = trg_arr[example_idx]\n",
        "\n",
        "translation = translate_sentence(model,src, device)\n",
        "print('src:',src)\n",
        "print('trg:',trg)\n",
        "print(f'predicted trg = {translation}')"
      ],
      "metadata": {
        "id": "1CJHtj8rN67H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9860216-72ef-4ff1-a1a6-90a3ec8e594c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src: Ein Mädchen in einem Jeanskleid läuft über einen erhöhten Schwebebalken.\n",
            "\n",
            "trg: A girl in a jean dress is walking along a raised balance beam.\n",
            "\n",
            "predicted trg = ['A', 'girl', 'in', 'a', 'dress', 'is', 'walking', 'over', 'a', 'stack', 'of', 'a', '.', '\\n', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_idx = 16\n",
        "src = src_arr[example_idx]\n",
        "trg = trg_arr[example_idx]\n",
        "\n",
        "translation = translate_sentence(model,src, device)\n",
        "print('src:',src)\n",
        "print('trg:',trg)\n",
        "print(f'predicted trg = {translation}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0T7nsbWdoAJ",
        "outputId": "d9b78ee9-18ed-4620-da6c-5b5842800835"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src: Eine Blondine hält mit einem Mann im Sand Händchen.\n",
            "\n",
            "trg: A blond holding hands with a guy in the sand.\n",
            "\n",
            "predicted trg = ['A', 'woman', 'holding', 'a', 'man', 'in', 'a', 'man', 'in', 'the', 'sand', '.', '\\n', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_idx = 17\n",
        "src = src_arr[example_idx]\n",
        "trg = trg_arr[example_idx]\n",
        "\n",
        "translation = translate_sentence(model,src, device)\n",
        "print('src:',src)\n",
        "print('trg:',trg)\n",
        "print(f'predicted trg = {translation}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKFGMBv9duEl",
        "outputId": "9c6a3e75-2b6b-460d-d448-27ca944bdb58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src: Eine Frau in einem Grauen Pulli und mit einer schwarzen Baseballmütze steht in einem Geschäft in der Schlange.\n",
            "\n",
            "trg: A woman in a gray sweater and black baseball cap is standing in line at a shop.\n",
            "\n",
            "predicted trg = ['A', 'woman', 'in', 'a', 'black', 'top', 'and', 'black', 'polka', 'dots', 'and', 'black', 'leggings', 'standing', 'in', 'a', 'in', 'a', 'store', '.', '\\n', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_idx = 19\n",
        "src = src_arr[example_idx]\n",
        "trg = trg_arr[example_idx]\n",
        "\n",
        "translation = translate_sentence(model,src, device)\n",
        "print('src:',src)\n",
        "print('trg:',trg)\n",
        "print(f'predicted trg = {translation}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xt7YpIIrdzuY",
        "outputId": "fbccd59d-b7f9-4b27-9967-e16fabd22222"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src: Zwei Männer tun so als seien sie Statuen, während Frauen ihnen zusehen.\n",
            "\n",
            "trg: Two men pretend to be statutes while women look on.\n",
            "\n",
            "predicted trg = ['Two', 'men', 'are', 'making', 'a', 'picture', 'while', 'while', 'while', 'them', 'watches', 'them', 'watches', 'them', '.', '\\n', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_idx = 20\n",
        "src = src_arr[example_idx]\n",
        "trg = trg_arr[example_idx]\n",
        "\n",
        "translation = translate_sentence(model,src, device)\n",
        "print('src:',src)\n",
        "print('trg:',trg)\n",
        "print(f'predicted trg = {translation}')"
      ],
      "metadata": {
        "id": "fLmL0_MYd7jw",
        "outputId": "99de8133-e255-4755-9c54-f82fc23d762d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "src: Leute, die vor einem Gebäude stehen.\n",
            "\n",
            "trg: People standing outside of a building.\n",
            "\n",
            "predicted trg = ['People', 'standing', 'in', 'front', 'building', 'near', 'a', 'building', '.', '\\n', '<eos>']\n"
          ]
        }
      ]
    }
  ]
}