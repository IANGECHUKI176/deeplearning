{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNj0HKKLZf14TC2AmFC63jh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IANGECHUKI176/deeplearning/blob/main/pytorch/convnets/EfficientNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "EfficientNet\n",
        "\n",
        "CNN models improves its ability to classify images by either increasing the depth of the network or\n",
        "by increasing the resolution of the images to capture finer details of the image or by increasing\n",
        "width of the network by increasing the number of channels. For instance, ResNet-18 to ResNet-152 has\n",
        "been built around these ideas.\n",
        "\n",
        "Now there is limit to each of these factors mentioned above and with increasing requirement of computational\n",
        "power. To overcome these challenges, researchers introducted the concept of compound scaling, which scales\n",
        "all the three factors moderately leading us to build EfficientNet.\n",
        "\n",
        "EfficientNet scales all the three factors i.e. depth, width and resolution but how to scale it? we can\n",
        "scale each factor equally but this wouldn't work if our task requires fine grained estimation and which\n",
        "requries more depth.\n",
        "\n",
        "Complex CNN architectures are built using multiple conv blocks and each block needs to be consistent with\n",
        "previous and next block, thus each layers in the block are scaled evenly.\n",
        "\n",
        "EfficientNet-B0 Architecture\n",
        "\n",
        "* Basic ConvNet Block (AlexNet)\n",
        "* Inverted Residual (MobileNetV2)\n",
        "* Squeeze and Excitation Block (Squeeze and Excitation Network)\n",
        "\n",
        "EfficientNet is a convolutional neural network architecture and scaling method that uniformly scales all\n",
        "dimensions of depth/width/resolution using a compound coefficient. Unlike conventional practice that arbitrary\n",
        "scales these factors, the EfficientNet scaling method uniformly scales network width, depth, and resolution\n",
        "with a set of fixed scaling coefficients. For example, if we want to use 2^N times more computational resources,\n",
        "then we can simply increase the network depth by alpha^N, width by beta^N, and image size by gamma^N, where\n",
        "alpha, beta and gamma, are constant coefficients determined by a small grid search on the original small model.\n",
        "EfficientNet uses a compound coefficient phi to uniformly scales network width, depth, and resolution in a\n",
        "principled way.\n",
        "\n",
        "The compound scaling method is justified by the intuition that if the input image is bigger, then the network\n",
        "needs more layers to increase the receptive field and more channels to capture more fine-grained patterns on\n",
        "the bigger image.\n",
        "\n",
        "The base EfficientNet-B0 network is based on the inverted bottleneck residual blocks of MobileNetV2, in addition\n",
        "to squeeze-and-excitation blocks.\n",
        "\n",
        "EfficientNets also transfer well and achieve state-of-the-art accuracy on CIFAR-100 (91.7%), Flowers (98.8%),\n",
        "and 3 other transfer learning datasets, with an order of magnitude fewer parameters.\n",
        "\n",
        "Interesting Stuff:\n",
        "\n",
        "Now, the most interesting part of EfficientNet-B0 is that the baseline architecture is designed by Neural\n",
        "Architecture Search(NAS). NAS is a wide topic and is not feasible to be discussed here. We can simply\n",
        "consider it as searching through the architecture space for underlying base architecture like ResNet or\n",
        "any other architecture for that matter. And on top of that, we can use grid search for finding the scale\n",
        "factor for Depth, Width and Resolution. Combining NAS and with compound scaling leads us to EfficientNet.\n",
        "Model is evaluated by comparing accuracy over the # of FLOPS(Floating point operations per second).\n",
        "\n",
        "Recommended Reading for NAS: https://lilianweng.github.io/lil-log/2020/08/06/neural-architecture-search.html"
      ],
      "metadata": {
        "id": "oVIv-5yAVO2A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "InLdASUGm9sr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = [\n",
        "    # expand_ratio, channels, repeats, stride, kernel_size\n",
        "    [1, 16, 1, 1, 3],\n",
        "    [6, 24, 2, 2, 3],\n",
        "    [6, 40, 2, 2, 5],\n",
        "    [6, 80, 3, 2, 3],\n",
        "    [6, 112, 3, 1, 5],\n",
        "    [6, 192, 4, 2, 5],\n",
        "    [6, 320, 1, 1, 3],\n",
        "]\n",
        "\n",
        "phi_values = {\n",
        "    # tuple of: (phi_value, resolution, drop_rate)\n",
        "    \"b0\": (0, 224, 0.2),  # alpha, beta, gamma, depth = alpha ** phi\n",
        "    \"b1\": (0.5, 240, 0.2),\n",
        "    \"b2\": (1, 260, 0.3),\n",
        "    \"b3\": (2, 300, 0.3),\n",
        "    \"b4\": (3, 380, 0.4),\n",
        "    \"b5\": (4, 456, 0.4),\n",
        "    \"b6\": (5, 528, 0.5),\n",
        "    \"b7\": (6, 600, 0.5),\n",
        "}"
      ],
      "metadata": {
        "id": "R2WLqlVHVUNT"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`swish` activation function == `nn.Silu`"
      ],
      "metadata": {
        "id": "vM0hvRD6V8BB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,kernel_size,stride,padding,groups = 1):\n",
        "        super(CNNBlock,self).__init__()\n",
        "        self.cnn = nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding,groups = groups,bias = False)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.silu = nn.SiLU()\n",
        "\n",
        "    def forward(self,x):\n",
        "        return self.silu(self.bn(self.cnn(x)))"
      ],
      "metadata": {
        "id": "sKxPKdSzVogM"
      },
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SqueezeAndExcitation(nn.Module):\n",
        "    def __init__(self,in_channels,reduced_dim):\n",
        "        super(SqueezeAndExcitation,self).__init__()\n",
        "\n",
        "        self.se = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1), # C x H x W -> C x 1 x 1\n",
        "            nn.Conv2d(in_channels,reduced_dim,1),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(reduced_dim,in_channels,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        return x * self.se(x)"
      ],
      "metadata": {
        "id": "TwC3JzTqX08E"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InvertedResidualBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels,\n",
        "        out_channels,\n",
        "        kernel_size,\n",
        "        stride,\n",
        "        padding,\n",
        "        expand_ratio,\n",
        "        reduction=4,  # squeeze excitation\n",
        "        survival_prob=0.8,  # for stochastic depth\n",
        "    ):\n",
        "        super(InvertedResidualBlock, self).__init__()\n",
        "        self.survival_prob = 0.8\n",
        "        self.use_residual = in_channels == out_channels and stride == 1\n",
        "        hidden_dim = in_channels * expand_ratio\n",
        "        self.expand = in_channels != hidden_dim\n",
        "        reduced_dim = int(in_channels / reduction)\n",
        "\n",
        "        if self.expand:\n",
        "            self.expand_conv = CNNBlock(\n",
        "                in_channels,\n",
        "                hidden_dim,\n",
        "                kernel_size=1,\n",
        "                stride=1,\n",
        "                padding=0,\n",
        "            )\n",
        "\n",
        "        self.conv = nn.Sequential(\n",
        "            CNNBlock(\n",
        "                hidden_dim,\n",
        "                hidden_dim,\n",
        "                kernel_size,\n",
        "                stride,\n",
        "                padding,\n",
        "                groups=hidden_dim,\n",
        "            ),\n",
        "            SqueezeAndExcitation(hidden_dim, reduced_dim),\n",
        "            nn.Conv2d(hidden_dim, out_channels, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "        )\n",
        "\n",
        "    def stochastic_depth(self, x):\n",
        "        if not self.training:\n",
        "            return x\n",
        "\n",
        "        binary_tensor = (\n",
        "            torch.rand(x.shape[0], 1, 1, 1, device=x.device) < self.survival_prob\n",
        "        )\n",
        "        return torch.div(x, self.survival_prob) * binary_tensor\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.expand_conv(inputs) if self.expand else inputs\n",
        "\n",
        "        if self.use_residual:\n",
        "            return self.stochastic_depth(self.conv(x)) + inputs\n",
        "        else:\n",
        "            return self.conv(x)"
      ],
      "metadata": {
        "id": "1s96Spo0ZRgD"
      },
      "execution_count": 162,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EfficientNet(nn.Module):\n",
        "    def __init__(self,version,n_classes):\n",
        "        super(EfficientNet,self).__init__()\n",
        "        width_factor,depth_factor,dropout_rate = self.calculate_factors(version)\n",
        "        print('factors',(width_factor,depth_factor,dropout_rate ))\n",
        "        last_channel = math.ceil(1280 * width_factor)\n",
        "        self.pool  = nn.AdaptiveAvgPool2d(1)\n",
        "        self.features = self.create_features(width_factor,depth_factor,last_channel)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(last_channel,n_classes)\n",
        "        )\n",
        "    def calculate_factors(self,version,alpha = 1.2,beta = 1.1):\n",
        "        phi,res,drop_rate = phi_values[version]\n",
        "        depth_factor = alpha ** phi\n",
        "        width_factor = beta ** phi\n",
        "        return width_factor,depth_factor ,drop_rate\n",
        "    def create_features(self,width_factor,depth_factor,last_channels):\n",
        "        channels = int(32*width_factor)\n",
        "\n",
        "        features = [CNNBlock(3, channels, 3, stride=2, padding=1)]\n",
        "        in_channels = channels\n",
        "        for expand_ratio, channels, repeats, stride, kernel_size in base_model:\n",
        "            out_channels =4* math.ceil(int(channels * width_factor)/4)\n",
        "            layers_repeats = math.ceil(repeats * depth_factor)\n",
        "            for layer in range(layers_repeats):\n",
        "                features.append(\n",
        "                    InvertedResidualBlock(in_channels,\n",
        "                 out_channels,\n",
        "                 kernel_size=kernel_size,\n",
        "                 stride = stride if layer == 0 else 1,\n",
        "                 padding = kernel_size // 2, # if k=1:pad=0, k=3:pad=1, k=5:pad=2\n",
        "                 expand_ratio = expand_ratio)\n",
        "                )\n",
        "                in_channels = out_channels\n",
        "        features.append(CNNBlock(in_channels,last_channels,kernel_size =1,stride = 1,padding = 0))\n",
        "        return nn.Sequential(*features)\n",
        "    def forward(self,x):\n",
        "        out = self.pool(self.features(x))\n",
        "        out = out.view(out.size(0),-1)\n",
        "        out = self.classifier(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "CrL7L6MwtpRQ"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "version = \"b0\"\n",
        "phi, res, drop_rate = phi_values[version]\n",
        "num_examples, num_classes = 4, 10\n",
        "x = torch.randn((num_examples, 3, res, res)).to(device)\n",
        "model = EfficientNet(\n",
        "        version=version,\n",
        "        n_classes=num_classes,\n",
        "    ).to(device)\n",
        "\n",
        "# print(model(x).shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Az0TQbj25e4V",
        "outputId": "d1b19af7-3585-4653-9883-e9f5c30e0dd4"
      },
      "execution_count": 164,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "factors (1.0, 1.0, 0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "\n",
        "summary(model,(3,224,224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scFgH2tG5pkr",
        "outputId": "7999ebcc-27ab-4377-c490-6d4bdbd66193"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1         [-1, 32, 112, 112]             864\n",
            "       BatchNorm2d-2         [-1, 32, 112, 112]              64\n",
            "              SiLU-3         [-1, 32, 112, 112]               0\n",
            "          CNNBlock-4         [-1, 32, 112, 112]               0\n",
            "            Conv2d-5         [-1, 32, 112, 112]             288\n",
            "       BatchNorm2d-6         [-1, 32, 112, 112]              64\n",
            "              SiLU-7         [-1, 32, 112, 112]               0\n",
            "          CNNBlock-8         [-1, 32, 112, 112]               0\n",
            " AdaptiveAvgPool2d-9             [-1, 32, 1, 1]               0\n",
            "           Conv2d-10              [-1, 8, 1, 1]             264\n",
            "             SiLU-11              [-1, 8, 1, 1]               0\n",
            "           Conv2d-12             [-1, 32, 1, 1]             288\n",
            "          Sigmoid-13             [-1, 32, 1, 1]               0\n",
            "SqueezeAndExcitation-14         [-1, 32, 112, 112]               0\n",
            "           Conv2d-15         [-1, 16, 112, 112]             512\n",
            "      BatchNorm2d-16         [-1, 16, 112, 112]              32\n",
            "InvertedResidualBlock-17         [-1, 16, 112, 112]               0\n",
            "           Conv2d-18         [-1, 96, 112, 112]           1,536\n",
            "      BatchNorm2d-19         [-1, 96, 112, 112]             192\n",
            "             SiLU-20         [-1, 96, 112, 112]               0\n",
            "         CNNBlock-21         [-1, 96, 112, 112]               0\n",
            "           Conv2d-22           [-1, 96, 56, 56]             864\n",
            "      BatchNorm2d-23           [-1, 96, 56, 56]             192\n",
            "             SiLU-24           [-1, 96, 56, 56]               0\n",
            "         CNNBlock-25           [-1, 96, 56, 56]               0\n",
            "AdaptiveAvgPool2d-26             [-1, 96, 1, 1]               0\n",
            "           Conv2d-27              [-1, 4, 1, 1]             388\n",
            "             SiLU-28              [-1, 4, 1, 1]               0\n",
            "           Conv2d-29             [-1, 96, 1, 1]             480\n",
            "          Sigmoid-30             [-1, 96, 1, 1]               0\n",
            "SqueezeAndExcitation-31           [-1, 96, 56, 56]               0\n",
            "           Conv2d-32           [-1, 24, 56, 56]           2,304\n",
            "      BatchNorm2d-33           [-1, 24, 56, 56]              48\n",
            "InvertedResidualBlock-34           [-1, 24, 56, 56]               0\n",
            "           Conv2d-35          [-1, 144, 56, 56]           3,456\n",
            "      BatchNorm2d-36          [-1, 144, 56, 56]             288\n",
            "             SiLU-37          [-1, 144, 56, 56]               0\n",
            "         CNNBlock-38          [-1, 144, 56, 56]               0\n",
            "           Conv2d-39          [-1, 144, 56, 56]           1,296\n",
            "      BatchNorm2d-40          [-1, 144, 56, 56]             288\n",
            "             SiLU-41          [-1, 144, 56, 56]               0\n",
            "         CNNBlock-42          [-1, 144, 56, 56]               0\n",
            "AdaptiveAvgPool2d-43            [-1, 144, 1, 1]               0\n",
            "           Conv2d-44              [-1, 6, 1, 1]             870\n",
            "             SiLU-45              [-1, 6, 1, 1]               0\n",
            "           Conv2d-46            [-1, 144, 1, 1]           1,008\n",
            "          Sigmoid-47            [-1, 144, 1, 1]               0\n",
            "SqueezeAndExcitation-48          [-1, 144, 56, 56]               0\n",
            "           Conv2d-49           [-1, 24, 56, 56]           3,456\n",
            "      BatchNorm2d-50           [-1, 24, 56, 56]              48\n",
            "InvertedResidualBlock-51           [-1, 24, 56, 56]               0\n",
            "           Conv2d-52          [-1, 144, 56, 56]           3,456\n",
            "      BatchNorm2d-53          [-1, 144, 56, 56]             288\n",
            "             SiLU-54          [-1, 144, 56, 56]               0\n",
            "         CNNBlock-55          [-1, 144, 56, 56]               0\n",
            "           Conv2d-56          [-1, 144, 28, 28]           3,600\n",
            "      BatchNorm2d-57          [-1, 144, 28, 28]             288\n",
            "             SiLU-58          [-1, 144, 28, 28]               0\n",
            "         CNNBlock-59          [-1, 144, 28, 28]               0\n",
            "AdaptiveAvgPool2d-60            [-1, 144, 1, 1]               0\n",
            "           Conv2d-61              [-1, 6, 1, 1]             870\n",
            "             SiLU-62              [-1, 6, 1, 1]               0\n",
            "           Conv2d-63            [-1, 144, 1, 1]           1,008\n",
            "          Sigmoid-64            [-1, 144, 1, 1]               0\n",
            "SqueezeAndExcitation-65          [-1, 144, 28, 28]               0\n",
            "           Conv2d-66           [-1, 40, 28, 28]           5,760\n",
            "      BatchNorm2d-67           [-1, 40, 28, 28]              80\n",
            "InvertedResidualBlock-68           [-1, 40, 28, 28]               0\n",
            "           Conv2d-69          [-1, 240, 28, 28]           9,600\n",
            "      BatchNorm2d-70          [-1, 240, 28, 28]             480\n",
            "             SiLU-71          [-1, 240, 28, 28]               0\n",
            "         CNNBlock-72          [-1, 240, 28, 28]               0\n",
            "           Conv2d-73          [-1, 240, 28, 28]           6,000\n",
            "      BatchNorm2d-74          [-1, 240, 28, 28]             480\n",
            "             SiLU-75          [-1, 240, 28, 28]               0\n",
            "         CNNBlock-76          [-1, 240, 28, 28]               0\n",
            "AdaptiveAvgPool2d-77            [-1, 240, 1, 1]               0\n",
            "           Conv2d-78             [-1, 10, 1, 1]           2,410\n",
            "             SiLU-79             [-1, 10, 1, 1]               0\n",
            "           Conv2d-80            [-1, 240, 1, 1]           2,640\n",
            "          Sigmoid-81            [-1, 240, 1, 1]               0\n",
            "SqueezeAndExcitation-82          [-1, 240, 28, 28]               0\n",
            "           Conv2d-83           [-1, 40, 28, 28]           9,600\n",
            "      BatchNorm2d-84           [-1, 40, 28, 28]              80\n",
            "InvertedResidualBlock-85           [-1, 40, 28, 28]               0\n",
            "           Conv2d-86          [-1, 240, 28, 28]           9,600\n",
            "      BatchNorm2d-87          [-1, 240, 28, 28]             480\n",
            "             SiLU-88          [-1, 240, 28, 28]               0\n",
            "         CNNBlock-89          [-1, 240, 28, 28]               0\n",
            "           Conv2d-90          [-1, 240, 14, 14]           2,160\n",
            "      BatchNorm2d-91          [-1, 240, 14, 14]             480\n",
            "             SiLU-92          [-1, 240, 14, 14]               0\n",
            "         CNNBlock-93          [-1, 240, 14, 14]               0\n",
            "AdaptiveAvgPool2d-94            [-1, 240, 1, 1]               0\n",
            "           Conv2d-95             [-1, 10, 1, 1]           2,410\n",
            "             SiLU-96             [-1, 10, 1, 1]               0\n",
            "           Conv2d-97            [-1, 240, 1, 1]           2,640\n",
            "          Sigmoid-98            [-1, 240, 1, 1]               0\n",
            "SqueezeAndExcitation-99          [-1, 240, 14, 14]               0\n",
            "          Conv2d-100           [-1, 80, 14, 14]          19,200\n",
            "     BatchNorm2d-101           [-1, 80, 14, 14]             160\n",
            "InvertedResidualBlock-102           [-1, 80, 14, 14]               0\n",
            "          Conv2d-103          [-1, 480, 14, 14]          38,400\n",
            "     BatchNorm2d-104          [-1, 480, 14, 14]             960\n",
            "            SiLU-105          [-1, 480, 14, 14]               0\n",
            "        CNNBlock-106          [-1, 480, 14, 14]               0\n",
            "          Conv2d-107          [-1, 480, 14, 14]           4,320\n",
            "     BatchNorm2d-108          [-1, 480, 14, 14]             960\n",
            "            SiLU-109          [-1, 480, 14, 14]               0\n",
            "        CNNBlock-110          [-1, 480, 14, 14]               0\n",
            "AdaptiveAvgPool2d-111            [-1, 480, 1, 1]               0\n",
            "          Conv2d-112             [-1, 20, 1, 1]           9,620\n",
            "            SiLU-113             [-1, 20, 1, 1]               0\n",
            "          Conv2d-114            [-1, 480, 1, 1]          10,080\n",
            "         Sigmoid-115            [-1, 480, 1, 1]               0\n",
            "SqueezeAndExcitation-116          [-1, 480, 14, 14]               0\n",
            "          Conv2d-117           [-1, 80, 14, 14]          38,400\n",
            "     BatchNorm2d-118           [-1, 80, 14, 14]             160\n",
            "InvertedResidualBlock-119           [-1, 80, 14, 14]               0\n",
            "          Conv2d-120          [-1, 480, 14, 14]          38,400\n",
            "     BatchNorm2d-121          [-1, 480, 14, 14]             960\n",
            "            SiLU-122          [-1, 480, 14, 14]               0\n",
            "        CNNBlock-123          [-1, 480, 14, 14]               0\n",
            "          Conv2d-124          [-1, 480, 14, 14]           4,320\n",
            "     BatchNorm2d-125          [-1, 480, 14, 14]             960\n",
            "            SiLU-126          [-1, 480, 14, 14]               0\n",
            "        CNNBlock-127          [-1, 480, 14, 14]               0\n",
            "AdaptiveAvgPool2d-128            [-1, 480, 1, 1]               0\n",
            "          Conv2d-129             [-1, 20, 1, 1]           9,620\n",
            "            SiLU-130             [-1, 20, 1, 1]               0\n",
            "          Conv2d-131            [-1, 480, 1, 1]          10,080\n",
            "         Sigmoid-132            [-1, 480, 1, 1]               0\n",
            "SqueezeAndExcitation-133          [-1, 480, 14, 14]               0\n",
            "          Conv2d-134           [-1, 80, 14, 14]          38,400\n",
            "     BatchNorm2d-135           [-1, 80, 14, 14]             160\n",
            "InvertedResidualBlock-136           [-1, 80, 14, 14]               0\n",
            "          Conv2d-137          [-1, 480, 14, 14]          38,400\n",
            "     BatchNorm2d-138          [-1, 480, 14, 14]             960\n",
            "            SiLU-139          [-1, 480, 14, 14]               0\n",
            "        CNNBlock-140          [-1, 480, 14, 14]               0\n",
            "          Conv2d-141          [-1, 480, 14, 14]          12,000\n",
            "     BatchNorm2d-142          [-1, 480, 14, 14]             960\n",
            "            SiLU-143          [-1, 480, 14, 14]               0\n",
            "        CNNBlock-144          [-1, 480, 14, 14]               0\n",
            "AdaptiveAvgPool2d-145            [-1, 480, 1, 1]               0\n",
            "          Conv2d-146             [-1, 20, 1, 1]           9,620\n",
            "            SiLU-147             [-1, 20, 1, 1]               0\n",
            "          Conv2d-148            [-1, 480, 1, 1]          10,080\n",
            "         Sigmoid-149            [-1, 480, 1, 1]               0\n",
            "SqueezeAndExcitation-150          [-1, 480, 14, 14]               0\n",
            "          Conv2d-151          [-1, 112, 14, 14]          53,760\n",
            "     BatchNorm2d-152          [-1, 112, 14, 14]             224\n",
            "InvertedResidualBlock-153          [-1, 112, 14, 14]               0\n",
            "          Conv2d-154          [-1, 672, 14, 14]          75,264\n",
            "     BatchNorm2d-155          [-1, 672, 14, 14]           1,344\n",
            "            SiLU-156          [-1, 672, 14, 14]               0\n",
            "        CNNBlock-157          [-1, 672, 14, 14]               0\n",
            "          Conv2d-158          [-1, 672, 14, 14]          16,800\n",
            "     BatchNorm2d-159          [-1, 672, 14, 14]           1,344\n",
            "            SiLU-160          [-1, 672, 14, 14]               0\n",
            "        CNNBlock-161          [-1, 672, 14, 14]               0\n",
            "AdaptiveAvgPool2d-162            [-1, 672, 1, 1]               0\n",
            "          Conv2d-163             [-1, 28, 1, 1]          18,844\n",
            "            SiLU-164             [-1, 28, 1, 1]               0\n",
            "          Conv2d-165            [-1, 672, 1, 1]          19,488\n",
            "         Sigmoid-166            [-1, 672, 1, 1]               0\n",
            "SqueezeAndExcitation-167          [-1, 672, 14, 14]               0\n",
            "          Conv2d-168          [-1, 112, 14, 14]          75,264\n",
            "     BatchNorm2d-169          [-1, 112, 14, 14]             224\n",
            "InvertedResidualBlock-170          [-1, 112, 14, 14]               0\n",
            "          Conv2d-171          [-1, 672, 14, 14]          75,264\n",
            "     BatchNorm2d-172          [-1, 672, 14, 14]           1,344\n",
            "            SiLU-173          [-1, 672, 14, 14]               0\n",
            "        CNNBlock-174          [-1, 672, 14, 14]               0\n",
            "          Conv2d-175          [-1, 672, 14, 14]          16,800\n",
            "     BatchNorm2d-176          [-1, 672, 14, 14]           1,344\n",
            "            SiLU-177          [-1, 672, 14, 14]               0\n",
            "        CNNBlock-178          [-1, 672, 14, 14]               0\n",
            "AdaptiveAvgPool2d-179            [-1, 672, 1, 1]               0\n",
            "          Conv2d-180             [-1, 28, 1, 1]          18,844\n",
            "            SiLU-181             [-1, 28, 1, 1]               0\n",
            "          Conv2d-182            [-1, 672, 1, 1]          19,488\n",
            "         Sigmoid-183            [-1, 672, 1, 1]               0\n",
            "SqueezeAndExcitation-184          [-1, 672, 14, 14]               0\n",
            "          Conv2d-185          [-1, 112, 14, 14]          75,264\n",
            "     BatchNorm2d-186          [-1, 112, 14, 14]             224\n",
            "InvertedResidualBlock-187          [-1, 112, 14, 14]               0\n",
            "          Conv2d-188          [-1, 672, 14, 14]          75,264\n",
            "     BatchNorm2d-189          [-1, 672, 14, 14]           1,344\n",
            "            SiLU-190          [-1, 672, 14, 14]               0\n",
            "        CNNBlock-191          [-1, 672, 14, 14]               0\n",
            "          Conv2d-192            [-1, 672, 7, 7]          16,800\n",
            "     BatchNorm2d-193            [-1, 672, 7, 7]           1,344\n",
            "            SiLU-194            [-1, 672, 7, 7]               0\n",
            "        CNNBlock-195            [-1, 672, 7, 7]               0\n",
            "AdaptiveAvgPool2d-196            [-1, 672, 1, 1]               0\n",
            "          Conv2d-197             [-1, 28, 1, 1]          18,844\n",
            "            SiLU-198             [-1, 28, 1, 1]               0\n",
            "          Conv2d-199            [-1, 672, 1, 1]          19,488\n",
            "         Sigmoid-200            [-1, 672, 1, 1]               0\n",
            "SqueezeAndExcitation-201            [-1, 672, 7, 7]               0\n",
            "          Conv2d-202            [-1, 192, 7, 7]         129,024\n",
            "     BatchNorm2d-203            [-1, 192, 7, 7]             384\n",
            "InvertedResidualBlock-204            [-1, 192, 7, 7]               0\n",
            "          Conv2d-205           [-1, 1152, 7, 7]         221,184\n",
            "     BatchNorm2d-206           [-1, 1152, 7, 7]           2,304\n",
            "            SiLU-207           [-1, 1152, 7, 7]               0\n",
            "        CNNBlock-208           [-1, 1152, 7, 7]               0\n",
            "          Conv2d-209           [-1, 1152, 7, 7]          28,800\n",
            "     BatchNorm2d-210           [-1, 1152, 7, 7]           2,304\n",
            "            SiLU-211           [-1, 1152, 7, 7]               0\n",
            "        CNNBlock-212           [-1, 1152, 7, 7]               0\n",
            "AdaptiveAvgPool2d-213           [-1, 1152, 1, 1]               0\n",
            "          Conv2d-214             [-1, 48, 1, 1]          55,344\n",
            "            SiLU-215             [-1, 48, 1, 1]               0\n",
            "          Conv2d-216           [-1, 1152, 1, 1]          56,448\n",
            "         Sigmoid-217           [-1, 1152, 1, 1]               0\n",
            "SqueezeAndExcitation-218           [-1, 1152, 7, 7]               0\n",
            "          Conv2d-219            [-1, 192, 7, 7]         221,184\n",
            "     BatchNorm2d-220            [-1, 192, 7, 7]             384\n",
            "InvertedResidualBlock-221            [-1, 192, 7, 7]               0\n",
            "          Conv2d-222           [-1, 1152, 7, 7]         221,184\n",
            "     BatchNorm2d-223           [-1, 1152, 7, 7]           2,304\n",
            "            SiLU-224           [-1, 1152, 7, 7]               0\n",
            "        CNNBlock-225           [-1, 1152, 7, 7]               0\n",
            "          Conv2d-226           [-1, 1152, 7, 7]          28,800\n",
            "     BatchNorm2d-227           [-1, 1152, 7, 7]           2,304\n",
            "            SiLU-228           [-1, 1152, 7, 7]               0\n",
            "        CNNBlock-229           [-1, 1152, 7, 7]               0\n",
            "AdaptiveAvgPool2d-230           [-1, 1152, 1, 1]               0\n",
            "          Conv2d-231             [-1, 48, 1, 1]          55,344\n",
            "            SiLU-232             [-1, 48, 1, 1]               0\n",
            "          Conv2d-233           [-1, 1152, 1, 1]          56,448\n",
            "         Sigmoid-234           [-1, 1152, 1, 1]               0\n",
            "SqueezeAndExcitation-235           [-1, 1152, 7, 7]               0\n",
            "          Conv2d-236            [-1, 192, 7, 7]         221,184\n",
            "     BatchNorm2d-237            [-1, 192, 7, 7]             384\n",
            "InvertedResidualBlock-238            [-1, 192, 7, 7]               0\n",
            "          Conv2d-239           [-1, 1152, 7, 7]         221,184\n",
            "     BatchNorm2d-240           [-1, 1152, 7, 7]           2,304\n",
            "            SiLU-241           [-1, 1152, 7, 7]               0\n",
            "        CNNBlock-242           [-1, 1152, 7, 7]               0\n",
            "          Conv2d-243           [-1, 1152, 7, 7]          28,800\n",
            "     BatchNorm2d-244           [-1, 1152, 7, 7]           2,304\n",
            "            SiLU-245           [-1, 1152, 7, 7]               0\n",
            "        CNNBlock-246           [-1, 1152, 7, 7]               0\n",
            "AdaptiveAvgPool2d-247           [-1, 1152, 1, 1]               0\n",
            "          Conv2d-248             [-1, 48, 1, 1]          55,344\n",
            "            SiLU-249             [-1, 48, 1, 1]               0\n",
            "          Conv2d-250           [-1, 1152, 1, 1]          56,448\n",
            "         Sigmoid-251           [-1, 1152, 1, 1]               0\n",
            "SqueezeAndExcitation-252           [-1, 1152, 7, 7]               0\n",
            "          Conv2d-253            [-1, 192, 7, 7]         221,184\n",
            "     BatchNorm2d-254            [-1, 192, 7, 7]             384\n",
            "InvertedResidualBlock-255            [-1, 192, 7, 7]               0\n",
            "          Conv2d-256           [-1, 1152, 7, 7]         221,184\n",
            "     BatchNorm2d-257           [-1, 1152, 7, 7]           2,304\n",
            "            SiLU-258           [-1, 1152, 7, 7]               0\n",
            "        CNNBlock-259           [-1, 1152, 7, 7]               0\n",
            "          Conv2d-260           [-1, 1152, 7, 7]          10,368\n",
            "     BatchNorm2d-261           [-1, 1152, 7, 7]           2,304\n",
            "            SiLU-262           [-1, 1152, 7, 7]               0\n",
            "        CNNBlock-263           [-1, 1152, 7, 7]               0\n",
            "AdaptiveAvgPool2d-264           [-1, 1152, 1, 1]               0\n",
            "          Conv2d-265             [-1, 48, 1, 1]          55,344\n",
            "            SiLU-266             [-1, 48, 1, 1]               0\n",
            "          Conv2d-267           [-1, 1152, 1, 1]          56,448\n",
            "         Sigmoid-268           [-1, 1152, 1, 1]               0\n",
            "SqueezeAndExcitation-269           [-1, 1152, 7, 7]               0\n",
            "          Conv2d-270            [-1, 320, 7, 7]         368,640\n",
            "     BatchNorm2d-271            [-1, 320, 7, 7]             640\n",
            "InvertedResidualBlock-272            [-1, 320, 7, 7]               0\n",
            "          Conv2d-273           [-1, 1280, 7, 7]         409,600\n",
            "     BatchNorm2d-274           [-1, 1280, 7, 7]           2,560\n",
            "            SiLU-275           [-1, 1280, 7, 7]               0\n",
            "        CNNBlock-276           [-1, 1280, 7, 7]               0\n",
            "AdaptiveAvgPool2d-277           [-1, 1280, 1, 1]               0\n",
            "         Dropout-278                 [-1, 1280]               0\n",
            "          Linear-279                   [-1, 10]          12,810\n",
            "================================================================\n",
            "Total params: 4,020,358\n",
            "Trainable params: 4,020,358\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 219.02\n",
            "Params size (MB): 15.34\n",
            "Estimated Total Size (MB): 234.93\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}