{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM6WvFNEk2BKEn7CCzo5xk4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IANGECHUKI176/deeplearning/blob/main/pytorch/convnets/rir.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# resnet in resnet in pytorch"
      ],
      "metadata": {
        "id": "ZWXu6w-cGkAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sasha Targ, Diogo Almeida, Kevin Lyman.\n",
        "\n",
        "Resnet in Resnet: Generalizing Residual Architectures\n",
        "\n",
        "> https://arxiv.org/abs/1603.08029v1"
      ],
      "metadata": {
        "id": "GLFACP_qGZ-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "JEHZnK1yk6cH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResnetInit(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels,stride):\n",
        "        super(ResnetInit,self).__init__()\n",
        "\n",
        "        #\"\"\"The modular unit of the generalized residual network architecture is a\n",
        "        #generalized residual block consisting of parallel states for a residual stream,\n",
        "        #r, which contains identity shortcut connections and is similar to the structure\n",
        "        #of a residual block from the original ResNet with a single convolutional layer\n",
        "        #(parameters W l,r→r )\n",
        "        self.residual_stream_conv = nn.Conv2d(in_channels,out_channels,3,padding = 1,stride = stride)\n",
        "        #\"\"\"and a transient stream, t, which is a standard convolutional layer\n",
        "        #(W l,t→t ).\"\"\"\n",
        "        self.transient_stream_conv = nn.Conv2d(in_channels,out_channels,3,padding = 1,stride = stride)\n",
        "        #\"\"\"Two additional sets of convolutional filters in each block (W l,r→t , W l,t→r )\n",
        "        #also transfer information across streams.\"\"\"\n",
        "\n",
        "        self.residual_stream_conv_across = nn.Conv2d(in_channels,out_channels,3,padding = 1,stride = stride)\n",
        "        #\"\"\"We use equal numbers of filters for the residual and transient streams of the\n",
        "        #generalized residual network, but optimizing this hyperparameter could lead to\n",
        "        #further potential improvements.\"\"\"\n",
        "        self.transient_stream_conv_across = nn.Conv2d(in_channels,out_channels,3,padding = 1,stride = stride)\n",
        "\n",
        "        self.residual_bn_relu = nn.Sequential(\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace = True)\n",
        "        )\n",
        "        self.transient_bn_relu = nn.Sequential(\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace = True)\n",
        "        )\n",
        "        #\"\"\"The form of the shortcut connection can be an identity function with\n",
        "        #the appropriate padding or a projection as in He et al. (2015b).\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels,out_channels,1,stride = stride)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x_residual,x_transient = x\n",
        "        residual_r_r = self.residual_stream_conv(x_residual)\n",
        "        residual_r_t = self.residual_stream_conv_across(x_residual)\n",
        "        residual_shortcut = self.shortcut(x_residual)\n",
        "\n",
        "        transient_t_t = self.transient_stream_conv(x_transient)\n",
        "        transient_t_r = self.transient_stream_conv_across(x_transient)\n",
        "\n",
        "        x_residual = self.residual_bn_relu(residual_r_r + transient_t_r + residual_shortcut)\n",
        "        x_transient = self.transient_bn_relu(transient_t_t + residual_r_t)\n",
        "        #\"\"\"Same-stream and cross-stream activations are summed (along with the\n",
        "        #shortcut connection for the residual stream) before applying batch\n",
        "        #normalization and ReLU nonlinearities (together σ) to get the output\n",
        "        #states of the block (Equation 1) (Ioffe & Szegedy, 2015).\"\"\"\n",
        "        return x_residual, x_transient"
      ],
      "metadata": {
        "id": "ho0cLCZ3G5WH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RiRBlock(nn.Module):\n",
        "    def __init__(self,in_channels,out_channel,layer_num,stride,layer = ResnetInit):\n",
        "        super(RiRBlock,self).__init__()\n",
        "        self.resnetinit = self._make_layers(in_channels,out_channel,layer_num,stride)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x_residual, x_transient = self.resnetinit(x)\n",
        "        return (x_residual, x_transient)\n",
        "    #\"\"\"Replacing each of the convolutional layers within a residual\n",
        "    #block from the original ResNet (Figure 1a) with a generalized residual block\n",
        "    #(Figure 1b) leads us to a new architecture we call ResNet in ResNet (RiR)\n",
        "    #(Figure 1d).\"\"\"\n",
        "    def _make_layers(self, in_channel, out_channel, layer_num, stride, layer=ResnetInit):\n",
        "        strides = [stride] + [1] * (layer_num - 1)\n",
        "        layers = nn.Sequential()\n",
        "        for index, s in enumerate(strides):\n",
        "            layers.add_module(\"generalized layers {}\".format(index), layer(in_channel, out_channel, s))\n",
        "            in_channel = out_channel\n",
        "\n",
        "        return layers"
      ],
      "metadata": {
        "id": "nd6lEdGPM2v7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blk2 = RiRBlock(32, 64, 2, 1)\n",
        "\n",
        "# Create a random input tensor of the desired shape (batch_size, channels, height, width)\n",
        "input_tensor = torch.randn(1, 32, 224, 224)\n",
        "\n",
        "# Print the summary of the model using the print function\n",
        "x_residual = torch.randn(1, 32, 224, 224)\n",
        "x_transient = torch.randn(1, 32, 224, 224)\n",
        "\n",
        "# Pass the inputs as a tuple to the model's forward method\n",
        "output_residual, output_transient = blk2((x_residual, x_transient))"
      ],
      "metadata": {
        "id": "9cAJCLjVT-MA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResnetInResnet(nn.Module):\n",
        "    def __init__(self,n_classes = 10):\n",
        "        super(ResnetInResnet,self).__init__()\n",
        "        base = int(96/ 2)\n",
        "\n",
        "        self.residual_pre_conv = nn.Sequential(\n",
        "            nn.Conv2d(3,base,3,padding = 1,bias = False),\n",
        "            nn.BatchNorm2d(base),\n",
        "            nn.ReLU(inplace = True)\n",
        "        )\n",
        "        self.transient_pre_conv = nn.Sequential(\n",
        "            nn.Conv2d(3,base,3,padding = 1,bias = False),\n",
        "            nn.BatchNorm2d(base),\n",
        "            nn.ReLU(inplace = True)\n",
        "        )\n",
        "        self.rir1 = RiRBlock(base, base, 2, 1)\n",
        "        self.rir2 = RiRBlock(base, base, 2, 1)\n",
        "        self.rir3 = RiRBlock(base, base*2, 2, 2)\n",
        "        self.rir4 = RiRBlock(base*2, base*2, 2, 1)\n",
        "        self.rir5 = RiRBlock(base*2, base*2, 2, 1)\n",
        "        self.rir6 = RiRBlock(base*2, base*4, 2, 2)\n",
        "        self.rir7 = RiRBlock(base*4, base*4, 2, 1)\n",
        "        self.rir8 = RiRBlock(base*4, base*4, 2, 1)\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(384,n_classes,3,stride = 2),\n",
        "            nn.BatchNorm2d(n_classes),\n",
        "            nn.ReLU(inplace  = True)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(7290,450),\n",
        "            nn.ReLU(inplace = True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(450,10)\n",
        "        )\n",
        "        self._weight_init()\n",
        "    def forward(self,x):\n",
        "        x_residual = self.residual_pre_conv(x)\n",
        "        x_transient = self.transient_pre_conv(x)\n",
        "        x_residual,x_transient = self.rir1((x_residual,x_transient))\n",
        "        x_residual,x_transient = self.rir2((x_residual,x_transient))\n",
        "        x_residual,x_transient = self.rir3((x_residual,x_transient))\n",
        "        x_residual,x_transient = self.rir4((x_residual,x_transient))\n",
        "        x_residual,x_transient = self.rir5((x_residual,x_transient))\n",
        "        x_residual,x_transient = self.rir6((x_residual,x_transient))\n",
        "        x_residual,x_transient = self.rir7((x_residual,x_transient))\n",
        "        x_residual,x_transient = self.rir8((x_residual,x_transient))\n",
        "        h = torch.cat([x_residual,x_transient],1)\n",
        "        h = self.conv1(h)\n",
        "        h = h.view(h.size(0),-1)\n",
        "        h = self.classifier(h)\n",
        "        return h\n",
        "    def _weight_init(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m,nn.Conv2d):\n",
        "                torch.nn.init.kaiming_normal_(m.weight)\n",
        "                if m.bias is not None:  # Check if the layer has biases before filling them\n",
        "                    m.bias.data.fill_(0.01)"
      ],
      "metadata": {
        "id": "eidfY3b-aHXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def resnet_in_resnet():\n",
        "    return ResnetInResnet()"
      ],
      "metadata": {
        "id": "3TsF4y8Bli25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blk4 = resnet_in_resnet()\n",
        "blk4(torch.randn(1, 3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5L4M55jc-lU",
        "outputId": "0e5ff66d-2fe0-4298-9675-9be18e6bb6f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.3725, -0.1105, -0.3854, -0.2425, -0.4050,  0.2341,  0.2290, -0.4518,\n",
              "          0.1459,  0.3158]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    }
  ]
}