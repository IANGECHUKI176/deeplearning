{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMVmXJFNaRxTa4Xqx8hFFxh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/IANGECHUKI176/deeplearning/blob/main/pytorch/convnets/DenseNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "8bE05v6naoF7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wEGDsM1Ndkei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DenseNet [paper](https://arxiv.org/pdf/1608.06993.pdf)"
      ],
      "metadata": {
        "id": "0hXXGToAdfoy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: This first part I used [lightning docs](https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/04-inception-resnet-densenet.html) to code the DenseNet"
      ],
      "metadata": {
        "id": "N3ijwcY4oo-W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Swo-0ArRzYD"
      },
      "outputs": [],
      "source": [
        "class DenseLayer(nn.Module):\n",
        "    def __init__(self,c_in,bn_size,growth_rate,act_fn):\n",
        "        \"\"\"\n",
        "            Inputs:\n",
        "                c_in - no of input channels\n",
        "                bn_size - Bottleneck_size (factor of growth size) for output of the 1x1 .Typically between 2 and 4\n",
        "                growth_rate - No of output channels for the 3x3 convolution\n",
        "                act_fn - activation class constructor (nn.ReLU)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.BatchNorm2d(c_in),\n",
        "            act_fn(),\n",
        "            nn.Conv2d(c_in,bn_size * growth_rate,kernel_size = 1,bias = False),\n",
        "            nn.BatchNorm2d(bn_size*growth_rate),\n",
        "            act_fn(),\n",
        "            nn.Conv2d(bn_size*growth_rate,growth_rate,kernel_size = 3,padding = 1,bias = False)\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        out = self.net(x)\n",
        "        out = torch.cat([out,x],dim = 1)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self,c_in,num_layers,bn_size,growth_rate,act_fn):\n",
        "        \"\"\"\n",
        "        Inputs:\n",
        "            c_in - no of in_channels\n",
        "            num_layers - no of dense layers to apply in block\n",
        "            bn_size - Bottleneck size to use in the dense layers\n",
        "            growth_rate - growth rate to use in the dense layers\n",
        "            act_fn - activation func to use in the dense layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        for layer_idx in range(num_layers):\n",
        "            layer_c_in = c_in + layer_idx * growth_rate\n",
        "            layers.append(DenseLayer(layer_c_in,bn_size,growth_rate,act_fn))\n",
        "        self.block = nn.Sequential(*layers)\n",
        "    def forward(self,x):\n",
        "        out = self.block(x)\n",
        "        return out"
      ],
      "metadata": {
        "id": "L3AXQLuDXYIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# blk = DenseBlock(32,6,2,16,nn.ReLU)\n",
        "# blk"
      ],
      "metadata": {
        "id": "KVppuNGaf0Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Skip connections enforces dimensions to match, they overcome this by linearly projecting the features\n",
        "if the dimensions do not match"
      ],
      "metadata": {
        "id": "sZZ71UX1qb2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransitionLayer(nn.Module):\n",
        "    def __init__(self,c_in,c_out,act_fn):\n",
        "        super().__init__()\n",
        "        self.transition = nn.Sequential(\n",
        "            nn.BatchNorm2d(c_in),\n",
        "            act_fn(),\n",
        "            nn.Conv2d(c_in,c_out,kernel_size = 1,bias = False),\n",
        "            nn.AvgPool2d(kernel_size = 2,stride = 2)\n",
        "        )\n",
        "    def forward(self,x):\n",
        "        return self.transition(x)"
      ],
      "metadata": {
        "id": "sn7d3AeuZ2yb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseNetLightning(nn.Module):\n",
        "    def __init__(self,num_classes = 10,num_layers = [6,6,6,6],bn_size = 2, growth_rate = 16,act_fn = nn.ReLU):\n",
        "        super().__init__()\n",
        "        self._init_parameters()\n",
        "\n",
        "        c_hidden = bn_size * growth_rate\n",
        "        self.input_net = nn.Sequential(\n",
        "            nn.Conv2d(3,out_channels = c_hidden,kernel_size =3,padding = 1)\n",
        "        )\n",
        "        blocks = []\n",
        "        for blk_index,no_layers in enumerate(num_layers):\n",
        "            blocks.append(DenseBlock(c_in =c_hidden,num_layers = no_layers,bn_size = bn_size,growth_rate = growth_rate,act_fn = act_fn))\n",
        "            c_hidden = c_hidden + no_layers * growth_rate\n",
        "            if blk_index < len(num_layers) - 1: # Don't apply transition layer on last block\n",
        "                blocks.append(TransitionLayer(c_in = c_hidden,c_out = c_hidden // 2,act_fn = act_fn))\n",
        "                c_hidden = c_hidden // 2\n",
        "\n",
        "        self.blocks = nn.Sequential(*blocks)\n",
        "        self.output_net = nn.Sequential(\n",
        "            nn.BatchNorm2d(c_hidden),\n",
        "            act_fn(),\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(c_hidden,num_classes)\n",
        "        )\n",
        "\n",
        "    def _init_parameters(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m,nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight,nonlinearity='relu')\n",
        "            elif isinstance(m,nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight,1)\n",
        "                nn.init.constant_(m.bias,0)\n",
        "    def forward(self,x):\n",
        "        out = self.input_net(x)\n",
        "        out = self.blocks(out)\n",
        "        out = self.output_net(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "yRdyKKrweVc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blk0 = DenseNetLightning()"
      ],
      "metadata": {
        "id": "AX61DZ7wzQc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from torchsummary import summary\n",
        "# summary(blk0,(3,224,224))"
      ],
      "metadata": {
        "id": "ZPoQ7pZmCzYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 2: used this [github page](https://github.com/Mayurji/Image-Classification-PyTorch/blob/main/DenseNet.py).This is the same thing  as part but in a slightly different way"
      ],
      "metadata": {
        "id": "Yw1-JYAgsZVI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> In ResNet, we see how the skip connection added as identity function from the inputs\n",
        "to interact with the Conv layers. But in DenseNet, we see instead of adding skip\n",
        "connection to Conv layers, we can append or concat the output of identity function\n",
        "with output of Conv layers.\n",
        "\n",
        "> In ResNet, it is little tedious to make the dimensions to match for adding the skip\n",
        "connection and Conv Layers, but it is much simpler in DenseNet, as we concat the\n",
        "both the X and Conv's output.\n",
        "\n",
        "> The key idea or the reason its called DenseNet is because the next layers not only get\n",
        "the input from previous layer but also preceeding layers before the previous layer. So\n",
        "the next layer becomes dense as it loaded with output from previous layers.\n",
        "\n",
        ">Two blocks comprise DenseNet, one is DenseBlock for concat operation and other is\n",
        "transition layer for controlling channels meaning dimensions (recall 1x1 Conv)."
      ],
      "metadata": {
        "id": "KYhWORt4tqA0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "1V52UEhFs1JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self,in_channels,growth_rate):\n",
        "        super(Bottleneck,self).__init__()\n",
        "        intermediate_channels = 4 * growth_rate # 4- bottleneck_size\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.conv1 = nn.Conv2d(in_channels,intermediate_channels,kernel_size=1,bias = False)\n",
        "        self.bn2 = nn.BatchNorm2d(intermediate_channels)\n",
        "        self.conv2 = nn.Conv2d(intermediate_channels,growth_rate,kernel_size = 3,padding = 1,bias = False)\n",
        "    def forward(self,x):\n",
        "        out = self.conv1(F.relu(self.bn1(x)))\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out = torch.cat((x,out),1)\n",
        "        return out"
      ],
      "metadata": {
        "id": "Ft_DspkXveAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SingleLayer(nn.Module):\n",
        "    def __init__(self,in_channels,growth_rate):\n",
        "        super(SingleLayer,self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.conv1 = nn.Conv2d(in_channels,growth_rate,kernel_size = 3,padding = 1,bias = False)\n",
        "    def forward(self,x):\n",
        "        out = self.conv1(F.relu(self.bn1(x)))\n",
        "        out = torch.cat((x,out),1)\n",
        "        return out"
      ],
      "metadata": {
        "id": "UkQtp3C7yAPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "for part 2 the transition layer is named:`Transition` but they are doing the same thing"
      ],
      "metadata": {
        "id": "_RW1vXLb2oNg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transition(nn.Module):\n",
        "    def __init__(self,in_channels,out_channels):\n",
        "        super(Transition,self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
        "        self.conv1 = nn.Conv2d(in_channels,out_channels,kernel_size = 1,bias = False)\n",
        "    def forward(self,x):\n",
        "        out = self.conv1(F.relu(self.bn1(x)))\n",
        "        out = F.avg_pool2d(out,2)\n",
        "        return out"
      ],
      "metadata": {
        "id": "TWiFBwgg2dqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseNet(nn.Module):\n",
        "    \"\"\"\n",
        "    nblocks - nDenseblocks here they are predefined,you can use the formular:\n",
        "                                                                            (depth-4) // 3\n",
        "                                                                            the pass depht as a paramter\n",
        "    \"\"\"\n",
        "    def __init__(self,block,nblocks,growth_rate = 12,reduction = 0.5,num_classes = 10):\n",
        "        super(DenseNet,self).__init__()\n",
        "        self.growth_rate = growth_rate\n",
        "        num_planes = 2*growth_rate\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3,num_planes,kernel_size = 3,padding =1 ,bias = False)\n",
        "\n",
        "        self.dense1= self._make_dense_layers(block,num_planes,nblocks[0])\n",
        "        num_planes += nblocks[0]*growth_rate\n",
        "        out_planes = int(math.floor(num_planes*reduction))\n",
        "        self.trans1 = Transition(num_planes,out_planes)\n",
        "        num_planes = out_planes\n",
        "\n",
        "        self.dense2 = self._make_dense_layers(block,num_planes,nblocks[1])\n",
        "        num_planes += nblocks[1]*growth_rate\n",
        "        out_planes = int(math.floor(num_planes * reduction))\n",
        "        self.trans2 = Transition(num_planes,out_planes)\n",
        "        num_planes = out_planes\n",
        "\n",
        "        self.dense3 = self._make_dense_layers(block,num_planes,nblocks[2])\n",
        "        num_planes += nblocks[2]*growth_rate\n",
        "        out_planes = int(math.floor(num_planes * reduction))\n",
        "        self.trans3= Transition(num_planes,out_planes)\n",
        "        num_planes = out_planes\n",
        "\n",
        "        self.dense4 = self._make_dense_layers(block,num_planes,nblocks[3])\n",
        "        num_planes += nblocks[3]*growth_rate\n",
        "\n",
        "\n",
        "        self.bn = nn.BatchNorm2d(num_planes)\n",
        "        self.linear = nn.Linear(num_planes,num_classes)\n",
        "    def _make_dense_layers(self,block,in_planes,nblock):\n",
        "        layers = []\n",
        "        for _ in range(nblock):\n",
        "            layers.append(block(in_planes,self.growth_rate))\n",
        "            in_planes += self.growth_rate\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.trans1(self.dense1(out))\n",
        "        out = self.trans2(self.dense2(out))\n",
        "        out = self.trans3(self.dense3(out))\n",
        "        out = self.dense4(out)\n",
        "\n",
        "        out = F.relu(self.bn(out))\n",
        "        out = F.avg_pool2d(out, out.size(-1))  # Global average pooling\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "NN6do8xq7EMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary"
      ],
      "metadata": {
        "id": "s_BxB315SdXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def DenseNet121():\n",
        "#     return DenseNet(Bottleneck, [2,2,2,2], growth_rate=32)\n",
        "# input = torch.randn(3,224,224)\n",
        "# blk1 = DenseNet121()\n",
        "\n",
        "# summary(blk1,(3,224,224))\n"
      ],
      "metadata": {
        "id": "FxSvDIW1KLI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`DenseNet2` is same as `DenseNet` above only that is uses a for loop which is more concise"
      ],
      "metadata": {
        "id": "3UXp1EUmYWNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DenseNet2(nn.Module):\n",
        "    \"\"\"\n",
        "    nblocks - nDenseblocks here they are predefined,you can use the formular:\n",
        "                                                                            (depth-4) // 3\n",
        "                                                                            the pass depht as a paramter\n",
        "    \"\"\"\n",
        "    def __init__(self,block,nblocks,growth_rate = 12,reduction = 0.5,num_classes = 10):\n",
        "        super(DenseNet2,self).__init__()\n",
        "        self.growth_rate = growth_rate\n",
        "        num_planes = 2*growth_rate\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3,num_planes,kernel_size = 3,padding =1 ,bias = False)\n",
        "        # use for loop to eliminate all this code\n",
        "        dense_blocks = []\n",
        "        for blk_index,block_count in enumerate(nblocks):\n",
        "            dense_layer = self._make_dense_layers(block,num_planes,block_count)\n",
        "            dense_blocks.append(dense_layer)\n",
        "            num_planes += growth_rate * block_count\n",
        "\n",
        "            if blk_index < len(nblocks) -1: # dont add translation layers after the last block\n",
        "                out_planes = int(math.floor(num_planes * reduction))\n",
        "                dense_blocks.append(Transition(num_planes,out_planes))\n",
        "                num_planes = out_planes\n",
        "        self.dense_blocks = nn.Sequential(*dense_blocks)\n",
        "        self.output_net = nn.Sequential(\n",
        "            nn.BatchNorm2d(num_planes),\n",
        "            nn.ReLU(),\n",
        "            nn.AdaptiveAvgPool2d((1,1)),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(num_planes,num_classes)\n",
        "        )\n",
        "    def _make_dense_layers(self,block,in_planes,nblock):\n",
        "        layers = []\n",
        "        for _ in range(nblock):\n",
        "            layers.append(block(in_planes,self.growth_rate))\n",
        "            in_planes += self.growth_rate\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.dense_blocks(out)\n",
        "        out = self.output_net(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "CEoo0R3KNVHu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def DenseNet121():\n",
        "#     return DenseNet2(Bottleneck, [2,2,2,2], growth_rate=32)\n",
        "\n",
        "# input = torch.randn(3,224,224)\n",
        "# blk2 = DenseNet121()\n",
        "# summary(blk2,(3,224,224))"
      ],
      "metadata": {
        "id": "kphhnALnQTvp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def DenseNet121():\n",
        "    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=32)\n",
        "\n",
        "def DenseNet169():\n",
        "    return DenseNet(Bottleneck, [6,12,32,32], growth_rate=32)\n",
        "\n",
        "def DenseNet201():\n",
        "    return DenseNet(Bottleneck, [6,12,48,32], growth_rate=32)\n",
        "\n",
        "def DenseNet161():\n",
        "    return DenseNet(Bottleneck, [6,12,36,24], growth_rate=48)\n",
        "\n",
        "def densenet_cifar():\n",
        "    return DenseNet(Bottleneck, [6,12,24,16], growth_rate=12)"
      ],
      "metadata": {
        "id": "22p_hyARZReh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}